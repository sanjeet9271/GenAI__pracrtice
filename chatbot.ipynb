{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip==23.1.2 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 1)) (23.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 5)) (2.18.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 6)) (3.9.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 7)) (4.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\skumar10\\appdata\\roaming\\python\\python311\\site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow->-r requirements.txt (line 5)) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\skumar10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\skumar10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\skumar10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (0.31.0)\n",
      "Requirement already satisfied: click in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->-r requirements.txt (line 6)) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->-r requirements.txt (line 6)) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk->-r requirements.txt (line 6)) (4.67.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim->-r requirements.txt (line 7)) (7.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\skumar10\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk->-r requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\skumar10\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 5)) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\skumar10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial intelligence o\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_text(text_data):\n",
    "    data = text_data.lower()\n",
    "    data = re.sub(r'[^a-zA-Z0-9\\s.]','',data)\n",
    "    return data\n",
    "\n",
    "content = \"\"\n",
    "with open('dataset.txt','r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "data = preprocess_text(content)\n",
    "print(data[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "[['artificial', 'intelligence', 'or', 'ai', 'is', 'the', 'technology', 'behind', 'the', 'fourth', 'industrial', 'revolution', 'that', 'has', 'brought', 'great', 'changes', 'all', 'around', 'the', 'world'], ['it', 'is', 'usually', 'defined', 'as', 'the', 'study', 'of', 'intelligent', 'systems', 'that', 'could', 'execute', 'tasks', 'and', 'activities', 'that', 'would', 'require', 'human', 'level', 'intelligence'], ['similar', 'to', 'the', 'past', 'three', 'industrial', 'revolutions', 'ai', 'is', 'leaving', 'an', 'incredible', 'impact', 'on', 'productivity'], ['artificial', 'intelligence', 'ai', 'revolution', 'the', 'ai', 'revolution', 'has', 'fundamentally', 'changed', 'the', 'ways', 'people', 'collect', 'and', 'process', 'data', 'as', 'well', 'as', 'transformed', 'business', 'operations', 'across', 'different', 'industries'], ['in', 'general', 'ai', 'systems', 'are', 'supported', 'by', 'three', 'major', 'aspects', 'which', 'are', 'domain', 'knowledge', 'data', 'generation', 'and', 'machine', 'learning'], ['domain', 'knowledge', 'denotes', 'the', 'understanding', 'and', 'expertise', 'of', 'the', 'real', 'life', 'scenario', 'on', 'why', 'and', 'how', 'we', 'need', 'to', 'engineer', 'task'], ['the', 'data', 'aspect', 'refers', 'to', 'the', 'process', 'of', 'preparing', 'databases', 'required', 'to', 'feed', 'on', 'to', 'the', 'learning', 'algorithms'], ['lastly', 'machine', 'learning', 'detects', 'the', 'patterns', 'from', 'the', 'training', 'data', 'predicts', 'and', 'performs', 'tasks', 'without', 'being', 'manually', 'or', 'explicitly', 'programmed'], ['three', 'key', 'attributes', 'of', 'ai', 'technology'], ['intelligent', 'decision', 'making', 'the', 'simulation', 'of', 'human', 'intelligence', 'by', 'machines', 'can', 'infer', 'fast', 'solution', 'for', 'the', 'problems', 'that', 'are', 'often', 'faced', 'by', 'humanity'], ['ai', 'is', 'backed', 'by', 'advanced', 'data', 'analytics', 'and', 'machine', 'learning', 'which', 'means', 'ai', 'can', 'learn', 'and', 'gain', 'new', 'insights', 'as', 'it', 'keeps', 'feeding', 'on', 'new', 'data'], ['with', 'proper', 'input', 'ai', 'could', 'come', 'up', 'with', 'prompt', 'and', 'accurate', 'decisions'], ['in', 'addition', 'to', 'that', 'the', 'intelligence', 'attribute', 'of', 'ai', 'promotes', 'productivity', 'and', 'reduces', 'dependency', 'on', 'human', 'support', 'which', 'makes', 'ai', 'highly', 'autonomous', 'and', 'convenient', 'tool', 'to', 'have'], ['intentionality', 'intentionality', 'is', 'often', 'deemed', 'as', 'the', 'technical', 'and', 'ontological', 'attributes', 'of', 'computer', 'programs', 'that', 'derived', 'from', 'the', 'algorithms', 'and', 'knowledge', 'engineering'], ['this', 'attribute', 'can', 'be', 'interpreted', 'as', 'ais', 'capability', 'of', 'delivering', 'insights', 'from', 'the', 'real', 'time', 'information', 'and', 'reacting', 'in', 'the', 'way', 'similar', 'to', 'its', 'creators', 'and', 'users'], ['the', 'responses', 'usually', 'strongly', 'reflect', 'the', 'social', 'context', 'that', 'creator', 'and', 'users', 'are', 'in'], ['additionally', 'with', 'development', 'of', 'data', 'ingestion', 'storage', 'capacity', 'processing', 'speed', 'and', 'analytic', 'techniques', 'ai', 'gets', 'more', 'capable', 'of', 'responding', 'to', 'the', 'issues', 'with', 'increasing', 'sophistication'], ['this', 'very', 'much', 'differentiates', 'ai', 'with', 'the', 'fundamental', 'function', 'of', 'machines', 'that', 'merely', 'carry', 'out', 'predetermined', 'routines'], ['adaptability', 'and', 'prediction', 'machine', 'learning', 'facilitates', 'ai', 'to', 'discover', 'the', 'pattern', 'of', 'the', 'data', 'that', 'were', 'previously', 'programmed', 'which', 'enables', 'ais', 'capability', 'of', 'making', 'its', 'own', 'change', 'as', 'circumstances', 'change'], ['the', 'attribute', 'of', 'adaptability', 'profoundly', 'enhances', 'ais', 'prediction', 'and', 'decision', 'making'], ['one', 'of', 'the', 'commonly', 'seen', 'examples', 'is', 'gmails', 'smart', 'compose', 'feature', 'which', 'offers', 'the', 'use', 'of', 'personalised', 'suggestions', 'as', 'users', 'typing', 'sentence'], ['it', 'illustrates', 'how', 'ai', 'adapts', 'to', 'ones', 'personal', 'writing', 'pattern', 'and', 'delivers', 'appropriate', 'suggestions'], ['ai', 'in', 'the', 'business', 'undoubtedly', 'the', 'artificial', 'intelligence', 'revolutions', 'had', 'profoundly', 'impacted', 'the', 'way', 'businesses', 'operate'], ['the', 'most', 'common', 'practises', 'are', 'the', 'automation', 'of', 'repetitive', 'tasks', 'that', 'require', 'less', 'human', 'input'], ['however', 'with', 'the', 'consistent', 'improvement', 'of', 'algorithms', 'ai', 'technology', 'is', 'no', 'longer', 'only', 'limited', 'to', 'the', 'capability', 'of', 'expanding', 'productivity', 'but', 'also', 'becomes', 'necessary', 'tool', 'in', 'engaging', 'customers', 'providing', 'service', 'excellence', 'and', 'driving', 'innovation'], ['here', 'are', 'several', 'industrial', 'scenarios', 'to', 'demonstrate', 'how', 'ai', 'transformed', 'the', 'nature', 'and', 'scope', 'of', 'business', 'activities'], ['contact', 'center', 'the', 'contact', 'center', 'has', 'evolved', 'significantly', 'over', 'the', 'years', 'and', 'has', 'become', 'more', 'sophisticated', 'thanks', 'to', 'the', 'use', 'of', 'ai', 'automation'], ['we', 'can', 'see', 'technological', 'advancement', 'of', 'contact', 'centers', 'in', 'the', 'form', 'of', 'chatbots', 'and', 'talkbots', 'that', 'enables', 'availability', 'and', 'instant', 'response', 'for', 'consumer', 'engagement', 'at', 'scale'], ['changing', 'the', 'strategies', 'to', 'engage', 'customers', 'with', 'ai', 'based', 'automation', 'vastly', 'boost', 'service', 'capability', 'and', 'reduce', 'service', 'failures', 'that', 'are', 'usually', 'caused', 'by', 'underperforming', 'agents', 'or', 'emotional', 'labour'], ['while', 'human', 'agents', 'require', 'frequent', 'and', 'regular', 'customer', 'service', 'training', 'to', 'maintain', 'the', 'service', 'quality', 'ai', 'talkbot', 'learns', 'from', 'every', 'customer', 'interaction', 'and', 'keeps', 'improving', 'to', 'provide', 'excellent', 'service', 'over', 'time'], ['this', 'very', 'much', 'reduces', 'labour', 'cost', 'associated', 'with', 'performance', 'evaluation', 'and', 'contact', 'center', 'training'], ['furthermore', 'ai', 'systems', 'in', 'contact', 'centers', 'such', 'as', 'talkbot', 'have', 'the', 'capability', 'to', 'be', 'customized', 'to', 'deliver', 'more', 'personal', 'experience', 'through', 'goal', 'driven', 'dialogues', 'based', 'on', 'the', 'customer', 'data', 'and', 'business', 'metric'], ['in', 'other', 'words', 'talkbots', 'can', 'easily', 'do', 'upselling', 'and', 'cross', 'selling', 'if', 'they', 'are', 'given', 'sufficient', 'information', 'about', 'the', 'customers', 'and', 'the', 'business', 'plan'], ['even', 'without', 'any', 'customer', 'care', 'training', 'talkbots', 'can', 'conduct', 'sentiment', 'analysis', 'from', 'the', 'conversation', 'and', 'unlock', 'the', 'hidden', 'customer', 'data', 'in', 'customer', 'voice', 'calls'], ['this', 'in', 'turn', 'provides', 'great', 'insights', 'for', 'future', 'planning'], ['also', 'compared', 'to', 'the', 'traditional', 'contact', 'center', 'ai', 'systems', 'show', 'stronger', 'capabilities', 'in', 'collecting', 'information', 'from', 'each', 'call', 'which', 'are', 'used', 'to', 'generate', 'the', 'report', 'in', 'more', 'intelligent', 'manner', 'and', 'with', 'better', 'insights'], ['ecommerce', 'nowadays', 'the', 'ecommerce', 'market', 'is', 'highly', 'saturated', 'and', 'competitive'], ['top', 'commerce', 'companies', 'heavily', 'rely', 'on', 'ai', 'technology', 'to', 'better', 'understand', 'their', 'customers', 'and', 'to', 'give', 'their', 'customers', 'better', 'service', 'in', 'order', 'to', 'remain', 'competitive', 'and', 'profitable'], ['intelligent', 'product', 'recommendation', 'is', 'one', 'of', 'the', 'typical', 'applications', 'of', 'ai', 'in', 'the', 'ecommerce', 'industry'], ['this', 'is', 'realtime', 'application', 'of', 'an', 'ai', 'algorithm', 'that', 'attempts', 'to', 'figure', 'out', 'customers', 'preference', 'based', 'on', 'their', 'previous', 'purchases', 'researches', 'and', 'consumption', 'habits'], ['the', 'collected', 'insights', 'enable', 'ecommerce', 'companies', 'to', 'personalize', 'product', 'recommendations', 'for', 'different', 'online', 'shoppers'], ['to', 'certain', 'extent', 'it', 'enhances', 'the', 'shopping', 'experience', 'and', 'potentially', 'boosts', 'sales'], ['however', 'if', 'the', 'ecommerces', 'overuses', 'intelligent', 'product', 'recommendation', 'and', 'adopts', 'an', 'aggressive', 'marketing', 'strategy', 'the', 'reverse', 'effect', 'might', 'happen'], ['beyond', 'the', 'function', 'of', 'personalization', 'ecommerce', 'businesses', 'also', 'leverage', 'ai', 'technology', 'to', 'support', 'customer', 'service', 'through', 'chatbots', 'and', 'talkbots', 'to', 'assist', 'them', 'with', 'customer', 'care', 'inventory', 'management', 'via', 'demand', 'forecasting', 'or', 'product', 'promotion'], ['logistics', 'and', 'supply', 'chain', 'the', 'use', 'of', 'artificial', 'intelligence', 'and', 'machine', 'learning', 'has', 'fundamentally', 'transformed', 'supply', 'chain', 'management', 'and', 'delivered', 'strong', 'optimization', 'of', 'capabilities', 'associated', 'with', 'accurate', 'management', 'high', 'productivity', 'low', 'operating', 'cost', 'and', 'quick', 'delivery'], ['for', 'example', 'with', 'the', 'capability', 'of', 'handling', 'big', 'data', 'the', 'ai', 'technology', 'could', 'be', 'used', 'to', 'automates', 'the', 'workflow', 'of', 'inventory', 'management'], ['parcels', 'could', 'be', 'packed', 'and', 'sorted', 'in', 'seamless', 'process', 'at', 'large', 'scale', 'which', 'would', 'largely', 'reduce', 'processing', 'time', 'and', 'minimize', 'human', 'error'], ['also', 'the', 'ai', 'system', 'can', 'forecast', 'market', 'demand', 'from', 'the', 'market', 'and', 'purchase', 'histories', 'facilitating', 'the', 'prediction', 'of', 'the', 'future', 'sales', 'and', 'providing', 'information', 'to', 'support', 'resource', 'allocation'], ['moreover', 'ai', 'algorithms', 'are', 'also', 'being', 'used', 'to', 'optimize', 'the', 'shipping', 'and', 'delivery', 'route', 'with', 'some', 'of', 'the', 'most', 'advanced', 'ones', 'even', 'involving', 'the', 'prediction', 'and', 'management', 'of', 'traffic', 'lights'], ['overall', 'in', 'the', 'information', 'and', 'data', 'driven', 'era', 'the', 'potential', 'of', 'ai', 'is', 'tremendous'], ['business', 'process', 'automation', 'could', 'reduce', 'stress', 'on', 'internal', 'productivity', 'and', 'decrease', 'reliance', 'on', 'human', 'support', 'while', 'at', 'the', 'same', 'time', 'increase', 'operational', 'cost', 'efficiency'], ['machine', 'learning', 'enables', 'the', 'company', 'to', 'delve', 'into', 'more', 'intelligent', 'approach', 'and', 'continually', 'drives', 'the', 'evolution', 'business', 'model'], ['companies', 'should', 'prepare', 'themselves', 'for', 'the', 'ai', 'revolution', 'wave', 'so', 'they', 'can', 'leverage', 'on', 'the', 'technology', 'to', 'achieve', 'the', 'optimal', 'operational', 'excellence'], []]\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess # type: ignore\n",
    "sentence_list = data.split('.')\n",
    "processed_data = [simple_preprocess(sentence) for sentence in sentence_list ] \n",
    "print(len(processed_data))\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0, 'and': 1, 'of': 2, 'to': 3, 'ai': 4, 'in': 5, 'with': 6, 'that': 7, 'data': 8, 'on': 9, 'is': 10, 'are': 11, 'as': 12, 'can': 13, 'service': 14, 'customer': 15, 'from': 16, 'learning': 17, 'human': 18, 'business': 19, 'which': 20, 'technology': 21, 'intelligence': 22, 'customers': 23, 'contact': 24, 'intelligent': 25, 'capability': 26, 'machine': 27, 'for': 28, 'by': 29, 'has': 30, 'ecommerce': 31, 'this': 32, 'also': 33, 'productivity': 34, 'management': 35, 'information': 36, 'more': 37, 'insights': 38, 'could': 39, 'process': 40, 'time': 41, 'talkbots': 42, 'center': 43, 'artificial': 44, 'product': 45, 'training': 46, 'or': 47, 'revolution': 48, 'be': 49, 'it': 50, 'automation': 51, 'prediction': 52, 'systems': 53, 'algorithms': 54, 'support': 55, 'attribute': 56, 'use': 57, 'their': 58, 'making': 59, 'knowledge': 60, 'companies': 61, 'used': 62, 'how': 63, 'better': 64, 'reduce': 65, 'cost': 66, 'ais': 67, 'at': 68, 'users': 69, 'transformed': 70, 'industrial': 71, 'usually': 72, 'enables': 73, 'tasks': 74, 'require': 75, 'based': 76, 'three': 77, 'an': 78, 'market': 79, 'processing': 80, 'way': 81, 'chatbots': 82, 'accurate': 83, 'even': 84, 'care': 85, 'input': 86, 'its': 87, 'keeps': 88, 'change': 89, 'most': 90, 'new': 91, 'future': 92, 'businesses': 93, 'profoundly': 94, 'capabilities': 95, 'enhances': 96, 'often': 97, 'one': 98, 'personal': 99, 'scale': 100, 'ones': 101, 'suggestions': 102, 'however': 103, 'they': 104, 'over': 105, 'if': 106, 'very': 107, 'much': 108, 'function': 109, 'excellence': 110, 'talkbot': 111, 'while': 112, 'providing': 113, 'centers': 114, 'out': 115, 'adaptability': 116, 'associated': 117, 'intentionality': 118, 'experience': 119, 'through': 120, 'have': 121, 'driven': 122, 'tool': 123, 'labour': 124, 'agents': 125, 'reduces': 126, 'pattern': 127, 'highly': 128, 'advanced': 129, 'demand': 130, 'great': 131, 'revolutions': 132, 'we': 133, 'supply': 134, 'similar': 135, 'would': 136, 'activities': 137, 'sales': 138, 'real': 139, 'delivery': 140, 'without': 141, 'being': 142, 'recommendation': 143, 'operational': 144, 'programmed': 145, 'domain': 146, 'leverage': 147, 'attributes': 148, 'competitive': 149, 'machines': 150, 'chain': 151, 'decision': 152, 'different': 153, 'fundamentally': 154, 'inventory': 155, 'analytic': 156, 'people': 157, 'changed': 158, 'ways': 159, 'sophistication': 160, 'creator': 161, 'storage': 162, 'increasing': 163, 'additionally': 164, 'capacity': 165, 'issues': 166, 'collect': 167, 'differentiates': 168, 'development': 169, 'responding': 170, 'capable': 171, 'across': 172, 'operations': 173, 'speed': 174, 'well': 175, 'gets': 176, 'techniques': 177, 'ingestion': 178, 'past': 179, 'fundamental': 180, 'world': 181, 'behind': 182, 'feature': 183, 'compose': 184, 'smart': 185, 'gmails': 186, 'fourth': 187, 'brought': 188, 'examples': 189, 'seen': 190, 'commonly': 191, 'changes': 192, 'all': 193, 'circumstances': 194, 'around': 195, 'own': 196, 'impact': 197, 'defined': 198, 'study': 199, 'previously': 200, 'execute': 201, 'were': 202, 'discover': 203, 'facilitates': 204, 'level': 205, 'routines': 206, 'predetermined': 207, 'carry': 208, 'merely': 209, 'leaving': 210, 'incredible': 211, 'context': 212, 'fast': 213, 'industries': 214, 'detects': 215, 'prompt': 216, 'performs': 217, 'predicts': 218, 'decisions': 219, 'addition': 220, 'patterns': 221, 'lastly': 222, 'social': 223, 'promotes': 224, 'feed': 225, 'dependency': 226, 'required': 227, 'makes': 228, 'databases': 229, 'up': 230, 'come': 231, 'proper': 232, 'manually': 233, 'feeding': 234, 'explicitly': 235, 'gain': 236, 'key': 237, 'learn': 238, 'means': 239, 'analytics': 240, 'backed': 241, 'humanity': 242, 'faced': 243, 'simulation': 244, 'problems': 245, 'solution': 246, 'autonomous': 247, 'convenient': 248, 'preparing': 249, 'expertise': 250, 'reflect': 251, 'strongly': 252, 'general': 253, 'responses': 254, 'infer': 255, 'creators': 256, 'supported': 257, 'major': 258, 'reacting': 259, 'aspects': 260, 'generation': 261, 'denotes': 262, 'understanding': 263, 'delivering': 264, 'interpreted': 265, 'refers': 266, 'life': 267, 'scenario': 268, 'engineering': 269, 'derived': 270, 'why': 271, 'programs': 272, 'computer': 273, 'ontological': 274, 'technical': 275, 'deemed': 276, 'need': 277, 'engineer': 278, 'task': 279, 'aspect': 280, 'offers': 281, 'optimal': 282, 'personalised': 283, 'top': 284, 'extent': 285, 'shopping': 286, 'potentially': 287, 'boosts': 288, 'ecommerces': 289, 'overuses': 290, 'adopts': 291, 'aggressive': 292, 'marketing': 293, 'strategy': 294, 'reverse': 295, 'effect': 296, 'might': 297, 'happen': 298, 'beyond': 299, 'personalization': 300, 'assist': 301, 'them': 302, 'via': 303, 'forecasting': 304, 'promotion': 305, 'logistics': 306, 'delivered': 307, 'strong': 308, 'optimization': 309, 'certain': 310, 'shoppers': 311, 'online': 312, 'application': 313, 'heavily': 314, 'rely': 315, 'understand': 316, 'give': 317, 'order': 318, 'remain': 319, 'profitable': 320, 'typical': 321, 'applications': 322, 'industry': 323, 'realtime': 324, 'algorithm': 325, 'recommendations': 326, 'attempts': 327, 'figure': 328, 'preference': 329, 'previous': 330, 'purchases': 331, 'researches': 332, 'consumption': 333, 'habits': 334, 'collected': 335, 'enable': 336, 'personalize': 337, 'high': 338, 'low': 339, 'operating': 340, 'company': 341, 'overall': 342, 'era': 343, 'potential': 344, 'tremendous': 345, 'stress': 346, 'internal': 347, 'decrease': 348, 'reliance': 349, 'same': 350, 'increase': 351, 'efficiency': 352, 'delve': 353, 'traffic': 354, 'into': 355, 'approach': 356, 'continually': 357, 'drives': 358, 'evolution': 359, 'model': 360, 'should': 361, 'prepare': 362, 'themselves': 363, 'wave': 364, 'so': 365, 'lights': 366, 'involving': 367, 'quick': 368, 'minimize': 369, 'example': 370, 'handling': 371, 'big': 372, 'automates': 373, 'workflow': 374, 'parcels': 375, 'packed': 376, 'sorted': 377, 'seamless': 378, 'large': 379, 'largely': 380, 'error': 381, 'some': 382, 'system': 383, 'forecast': 384, 'purchase': 385, 'histories': 386, 'facilitating': 387, 'resource': 388, 'allocation': 389, 'moreover': 390, 'optimize': 391, 'shipping': 392, 'route': 393, 'commerce': 394, 'saturated': 395, 'typing': 396, 'nowadays': 397, 'several': 398, 'scenarios': 399, 'demonstrate': 400, 'nature': 401, 'scope': 402, 'evolved': 403, 'significantly': 404, 'years': 405, 'become': 406, 'sophisticated': 407, 'thanks': 408, 'see': 409, 'technological': 410, 'advancement': 411, 'form': 412, 'availability': 413, 'instant': 414, 'response': 415, 'consumer': 416, 'engagement': 417, 'changing': 418, 'strategies': 419, 'engage': 420, 'vastly': 421, 'boost': 422, 'here': 423, 'innovation': 424, 'driving': 425, 'practises': 426, 'sentence': 427, 'illustrates': 428, 'adapts': 429, 'writing': 430, 'delivers': 431, 'appropriate': 432, 'undoubtedly': 433, 'had': 434, 'impacted': 435, 'operate': 436, 'common': 437, 'repetitive': 438, 'engaging': 439, 'less': 440, 'consistent': 441, 'improvement': 442, 'no': 443, 'longer': 444, 'achieve': 445, 'limited': 446, 'expanding': 447, 'but': 448, 'becomes': 449, 'necessary': 450, 'failures': 451, 'caused': 452, 'underperforming': 453, 'turn': 454, 'about': 455, 'plan': 456, 'any': 457, 'conduct': 458, 'sentiment': 459, 'analysis': 460, 'conversation': 461, 'unlock': 462, 'hidden': 463, 'voice': 464, 'calls': 465, 'provides': 466, 'given': 467, 'planning': 468, 'compared': 469, 'traditional': 470, 'show': 471, 'stronger': 472, 'collecting': 473, 'each': 474, 'call': 475, 'generate': 476, 'report': 477, 'manner': 478, 'sufficient': 479, 'selling': 480, 'emotional': 481, 'evaluation': 482, 'frequent': 483, 'regular': 484, 'maintain': 485, 'quality': 486, 'learns': 487, 'every': 488, 'interaction': 489, 'improving': 490, 'provide': 491, 'excellent': 492, 'performance': 493, 'furthermore': 494, 'cross': 495, 'such': 496, 'customized': 497, 'deliver': 498, 'goal': 499, 'dialogues': 500, 'metric': 501, 'other': 502, 'words': 503, 'easily': 504, 'do': 505, 'upselling': 506, 'only': 507}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec# type: ignore\n",
    "text_model = Word2Vec(sentences = processed_data,vector_size=150,window=6,min_count = 1,workers=4)\n",
    "print(text_model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-5.6114937e-03  4.3097697e-03 -3.8408497e-03 ...  5.8945953e-03\n",
      "    1.7309022e-03 -6.1021456e-03]\n",
      "  [ 3.1982348e-03 -2.4926751e-03 -2.8242557e-03 ... -2.2744909e-03\n",
      "    1.0909910e-03 -1.1109323e-03]\n",
      "  [ 3.4287085e-03  6.5755905e-03 -3.7678396e-03 ... -3.3793645e-03\n",
      "    4.2594634e-03 -5.0210212e-03]\n",
      "  [ 5.4578749e-03 -3.1172938e-03 -6.5223430e-04 ... -2.4842825e-03\n",
      "    6.6838604e-03 -2.6170630e-04]\n",
      "  [-4.8180097e-03  2.6779200e-03  1.4840675e-03 ... -4.2007552e-03\n",
      "    5.8434573e-03  1.9768374e-03]]\n",
      "\n",
      " [[-2.2927455e-04 -1.6876333e-04  3.5219395e-03 ... -2.0152486e-03\n",
      "    2.2747007e-03 -6.6193182e-04]\n",
      "  [ 5.5461884e-03  6.3716522e-03  6.4699394e-03 ...  1.1852179e-03\n",
      "    6.6721807e-03 -4.4288570e-03]\n",
      "  [ 1.4054094e-04  2.1511207e-03 -1.1068784e-03 ... -5.0467048e-03\n",
      "    1.5063875e-03 -3.4113324e-03]\n",
      "  [-2.2927455e-04 -1.6876333e-04  3.5219395e-03 ... -2.0152486e-03\n",
      "    2.2747007e-03 -6.6193182e-04]\n",
      "  [-4.4888114e-03 -1.9015565e-03 -2.2925909e-03 ... -4.5638601e-03\n",
      "   -5.8922130e-03  4.1208835e-03]]\n",
      "\n",
      " [[-1.3124713e-03 -2.4729262e-03  3.0757759e-03 ...  2.0376558e-03\n",
      "   -3.4358890e-03  3.5123448e-03]\n",
      "  [-4.9000904e-03  4.3475074e-03 -2.2487065e-03 ... -2.2784855e-04\n",
      "    2.1502792e-03 -7.0243659e-03]\n",
      "  [-3.4949929e-03 -1.0382687e-03 -4.8453500e-03 ... -2.5198865e-03\n",
      "   -2.7131415e-03 -2.2758103e-03]\n",
      "  [ 5.3412463e-03  5.2516754e-03 -3.6032607e-03 ...  1.2208686e-03\n",
      "    1.9951838e-03  1.3017401e-03]\n",
      "  [ 3.7324054e-03 -1.5085051e-03 -6.0182591e-03 ... -6.1227395e-03\n",
      "    9.4025029e-04 -7.1297045e-04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 4.6213693e-03  3.1596899e-04  2.6341327e-03 ...  2.9617799e-03\n",
      "   -5.7592709e-03  3.7716243e-03]\n",
      "  [ 3.4471825e-03 -3.0064092e-03 -6.1450712e-03 ...  3.7292053e-03\n",
      "   -4.0417905e-03 -3.0122703e-04]\n",
      "  [-2.2927455e-04 -1.6876333e-04  3.5219395e-03 ... -2.0152486e-03\n",
      "    2.2747007e-03 -6.6193182e-04]\n",
      "  [ 5.4578749e-03 -3.1172938e-03 -6.5223430e-04 ... -2.4842825e-03\n",
      "    6.6838604e-03 -2.6170630e-04]\n",
      "  [-4.9000904e-03  4.3475074e-03 -2.2487065e-03 ... -2.2784855e-04\n",
      "    2.1502792e-03 -7.0243659e-03]]\n",
      "\n",
      " [[ 3.9811144e-03 -1.8584825e-03  1.0775074e-03 ... -4.6066069e-03\n",
      "   -3.7563052e-03  3.0796600e-03]\n",
      "  [ 1.0097994e-03 -2.0394821e-03 -4.6913661e-03 ...  4.1669528e-03\n",
      "    2.1704894e-03  1.6193577e-03]\n",
      "  [ 1.8326269e-03 -9.0973219e-04 -5.5753156e-03 ... -6.3287462e-03\n",
      "    4.3676426e-03  1.1578094e-03]\n",
      "  [-6.2176120e-03 -3.4268606e-03 -6.4825700e-03 ... -1.5280334e-03\n",
      "   -5.8424473e-03 -2.5127090e-03]\n",
      "  [ 5.3267635e-05  7.5177394e-04 -1.2958478e-03 ... -4.3960852e-03\n",
      "    6.2432843e-03 -2.4967375e-03]]\n",
      "\n",
      " [[-4.0468789e-05  5.1962968e-04 -4.6813381e-03 ...  5.1441281e-03\n",
      "   -3.6148357e-03  5.7004611e-03]\n",
      "  [-2.2927455e-04 -1.6876333e-04  3.5219395e-03 ... -2.0152486e-03\n",
      "    2.2747007e-03 -6.6193182e-04]\n",
      "  [ 5.5461884e-03  6.3716522e-03  6.4699394e-03 ...  1.1852179e-03\n",
      "    6.6721807e-03 -4.4288570e-03]\n",
      "  [ 2.7898480e-03  2.6813415e-03  6.7174537e-03 ... -6.4940942e-03\n",
      "    2.8385301e-03 -5.6323903e-03]\n",
      "  [-8.3804311e-04  4.1499254e-03 -3.6963532e-03 ... -2.7567351e-03\n",
      "   -1.5595084e-03 -3.8440698e-03]]]\n"
     ]
    }
   ],
   "source": [
    "input_vector_sequence = []\n",
    "window_size = 5\n",
    "input_processed_data = [item for sublist in processed_data for item in sublist]\n",
    "\n",
    "for index in range(0, len(input_processed_data), window_size):\n",
    "\n",
    "    seq = input_processed_data[index:index+window_size]\n",
    "\n",
    "    if len(seq) == window_size and all(word in text_model.wv for word in seq):\n",
    "        input_vector_sequence.append([text_model.wv[word] for word in seq])\n",
    "\n",
    "final_input = np.array(input_vector_sequence)\n",
    "\n",
    "print(final_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226, 5, 150)\n",
      "1134\n",
      "225\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "print(final_input.shape)\n",
    "print(len(input_processed_data))\n",
    "\n",
    "X = [seq[:-1] for seq in final_input][:-1]\n",
    "Y = [seq[-1] for seq in final_input][:-1]\n",
    "\n",
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now we have our inputs ready we can build transformer for chatbot```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 4, 150)\n",
      "(225, 150)\n",
      "Epoch 1/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0035 - loss: -0.0050  \n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0028 - loss: -0.0029   \n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0439 - loss: -0.0047\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0722 - loss: -0.0074\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0930 - loss: -0.0098\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0646 - loss: -0.0159\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0772 - loss: -0.0222\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1040 - loss: -0.0350\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1013 - loss: -0.0609\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1013 - loss: -0.1257\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1013 - loss: -0.2249\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0121 - loss: -0.3312\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: -0.4309\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: -0.5358\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: -0.6305\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: -0.7196\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: -0.8157\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: -0.8964\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: -0.9643\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: -1.0125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fdf60e1910>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, Dropout, Input, Dense, LayerNormalization, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, ff_size, embedding_dim, num_heads=3, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_size, activation='relu'), Dense(embedding_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def create_model(embed_dim, num_heads, ff_dim, input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    block = TransformerBlock(embedding_dim=embed_dim, num_heads=num_heads, ff_size=ff_dim)\n",
    "    x = block(inputs, training=True)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(20, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "window_size =4 # Sequence length\n",
    "embed_dim = 150  # Embedding dimension\n",
    "num_heads = 3  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "input_shape = (window_size, embed_dim)  # Shape of the input\n",
    "num_classes = 150  # Number of classes (same as embedding dimension)\n",
    "\n",
    "# Preparing X and Y\n",
    "X = np.array([seq[:-1] for seq in final_input][:-1])\n",
    "Y = np.array([seq[-1] for seq in final_input][:-1])\n",
    "\n",
    "print(X.shape)  # Should print (225, 4, 150)\n",
    "print(Y.shape)  # Should print (225, 150)\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y)).batch(batch_size)\n",
    "\n",
    "model = create_model(embed_dim, num_heads, ff_dim, input_shape=input_shape)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model using the dataset\n",
    "model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Next word prediction: knowledge\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(model, word2vec_model, text, window_size):\n",
    "    tokenized_text = simple_preprocess(text)\n",
    "    sequence = np.array([word2vec_model.wv[word] for word in tokenized_text[-window_size:] if word in word2vec_model.wv])\n",
    "    if len(sequence) < window_size:\n",
    "        sequence = np.pad(sequence, ((window_size - len(sequence), 0), (0, 0)), mode='constant')\n",
    "    sequence = sequence.reshape(1, window_size, -1)\n",
    "    preds = model.predict(sequence)\n",
    "    next_word_vector = preds[0]\n",
    "    next_word = word2vec_model.wv.similar_by_vector(next_word_vector, topn=1)[0][0]\n",
    "    return next_word\n",
    "\n",
    "input_text = \"pattern of the data that were previously\"\n",
    "predicted_word = predict_next_word(model, text_model, input_text, window_size)\n",
    "print(f\"Next word prediction: {predicted_word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
